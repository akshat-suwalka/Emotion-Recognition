{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr4gZMRuNetf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import required modules\n",
        "import cv2 as cv\n",
        "import math\n",
        "import time\n",
        "import argparse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4azMwREFaLQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getFaceBox(net, frame, conf_threshold=0.7):\n",
        "    frameOpencvDnn = frame.copy()\n",
        "    frameHeight = frameOpencvDnn.shape[0]\n",
        "    frameWidth = frameOpencvDnn.shape[1]\n",
        "    blob = cv.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
        "\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "    bboxes = []\n",
        "    for i in range(detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        if confidence > conf_threshold:\n",
        "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
        "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
        "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
        "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
        "            bboxes.append([x1, y1, x2, y2])\n",
        "            cv.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
        "    return frameOpencvDnn, bboxes\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNWEmw03aOn8",
        "colab_type": "code",
        "outputId": "a1c8e1a3-7747-4d50-82cb-2c74d69469b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "parser = argparse.ArgumentParser(description='Use this script to run age and gender recognition using OpenCV.')\n",
        "parser.add_argument('--input', help='Path to input image or video file. Skip this argument to capture frames from a camera.')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--input'], dest='input', nargs=None, const=None, default=None, type=None, choices=None, help='Path to input image or video file. Skip this argument to capture frames from a camera.', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z9JzeQkaR3h",
        "colab_type": "code",
        "outputId": "3c5541fd-fff9-447a-fdc3-a94655368ac5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "args = parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--input INPUT]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-1b7adf42-dacc-4359-8787-54adceb6b44f.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSpMqVZEaWcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "faceProto = 'opencv_face_detector.pbtxt'\n",
        "faceModel = 'opencv_face_detector_uint8.pb'\n",
        "\n",
        "ageProto = 'age_deploy.prototxt'\n",
        "ageModel = 'age_net.caffemodel'\n",
        "\n",
        "genderProto = 'gender_deploy.prototxt'\n",
        "genderModel = 'gender_net.caffemodel'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQLQGiRDaaHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
        "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
        "genderList = ['Male', 'Female']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyiXtUb7acnf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load network\n",
        "ageNet = cv.dnn.readNet(ageModel, ageProto)\n",
        "genderNet = cv.dnn.readNet(genderModel, genderProto)\n",
        "faceNet = cv.dnn.readNet(faceModel, faceProto)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7e0RJKVae08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Open a video file or an image file or a camera stream\n",
        "cap = cv.VideoCapture(0)#args.input if args.input else 0)\n",
        "padding = 20\n",
        "while cv.waitKey(1) < 0:\n",
        "    # Read frame\n",
        "    \n",
        "    t = time.time()\n",
        "    hasFrame, frame = cap.read()\n",
        "    if not hasFrame:\n",
        "        cv.waitKey()\n",
        "        break\n",
        "\n",
        "    frameFace, bboxes = getFaceBox(faceNet, frame)\n",
        "    if not bboxes:\n",
        "        print(\"No face Detected, Checking next frame\")\n",
        "        continue\n",
        "\n",
        "    for bbox in bboxes:\n",
        "        # print(bbox)\n",
        "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
        "\n",
        "        blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
        "        genderNet.setInput(blob)\n",
        "        genderPreds = genderNet.forward()\n",
        "        gender = genderList[genderPreds[0].argmax()]\n",
        "        # print(\"Gender Output : {}\".format(genderPreds))\n",
        "        print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
        "\n",
        "        ageNet.setInput(blob)\n",
        "        agePreds = ageNet.forward()\n",
        "        age = ageList[agePreds[0].argmax()]\n",
        "        print(\"Age Output : {}\".format(agePreds))\n",
        "        print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
        "\n",
        "        label = \"{},{}\".format(gender, age)\n",
        "        cv.putText(frameFace, label, (bbox[0], bbox[1]-10), cv.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv.LINE_AA)\n",
        "        cv.imshow(\"Age Gender Demo\", frameFace)\n",
        "        # cv.imwrite(\"age-gender-out-{}\".format(args.input),frameFace)\n",
        "    print(\"time : {:.3f}\".format(time.time() - t))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYQzXpRdm51p",
        "colab_type": "code",
        "outputId": "7f5331cd-31ac-4f06-9e02-4aa9e871551d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "!curl -o logo.png https://colab.research.google.com/img/colab_favicon_256px.png\n",
        "\n",
        "import cv2\n",
        "\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while(True):\n",
        "    # Capture frame-by-frame\n",
        "    ret, frame = cap.read()\n",
        "    print(frame)\n",
        "    # Our operations on the frame come here\n",
        "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2_imshow(frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# When everything done, release the capture\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5500  100  5500    0     0   107k      0 --:--:-- --:--:-- --:--:--  107k\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-04b88e13cfa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Display the resulting frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m0xFF\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"
          ]
        }
      ]
    }
  ]
}